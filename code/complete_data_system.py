#!/usr/bin/env python3
"""
å®Œæ•´çš„æ•°æ®åº“è®¾ç½®å’Œåˆå§‹åŒ–è„šæœ¬
"""

import os
import sys
import sqlite3
import json
import logging
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MedicalDatabase:
    """åŒ»å­¦æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, db_path="data/msd_medical_knowledge.db"):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        self.db_path = db_path
        self.db_dir = Path(db_path).parent
        self.db_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“å’Œè¡¨ç»“æ„"""
        conn = self.get_connection()
        try:
            self._create_tables(conn)
            self._create_indexes(conn)
            self._create_functions(conn)
            logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        finally:
            conn.close()
    
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return sqlite3.connect(self.db_path, check_same_thread=False)
    
    def _create_tables(self, conn):
        """åˆ›å»ºæ‰€æœ‰è¡¨"""
        
        # æ–‡ç« è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT UNIQUE NOT NULL,
                title TEXT NOT NULL,
                subtitle TEXT,
                category TEXT,
                subcategory TEXT,
                content TEXT,
                content_html TEXT,
                excerpt TEXT,
                version TEXT DEFAULT 'home',
                language TEXT DEFAULT 'en',
                author TEXT,
                last_reviewed TEXT,
                published_date TEXT,
                word_count INTEGER DEFAULT 0,
                content_hash TEXT UNIQUE,
                quality_score INTEGER DEFAULT 0,
                extracted_at TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åŒ»å­¦æœ¯è¯­è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS medical_terms (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                term TEXT NOT NULL,
                definition TEXT,
                synonyms TEXT,  -- JSON array
                category TEXT,  -- disease, drug, symptom, procedure, anatomy
                icd_code TEXT,
                umls_id TEXT,
                frequency_score REAL DEFAULT 0.0,
                related_articles TEXT,  -- JSON array of article IDs
                version TEXT DEFAULT 'home',
                language TEXT DEFAULT 'en',
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(term, version, language)
            )
        ''')
        
        # æ–‡ç« æœ¯è¯­å…³è”è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS article_terms (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                article_id INTEGER NOT NULL,
                term_id INTEGER NOT NULL,
                frequency INTEGER DEFAULT 1,
                positions TEXT,  -- JSON array of positions
                FOREIGN KEY (article_id) REFERENCES articles (id),
                FOREIGN KEY (term_id) REFERENCES medical_terms (id),
                UNIQUE(article_id, term_id)
            )
        ''')
        
        # è¯ç‰©ä¿¡æ¯è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS drugs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                generic_name TEXT,
                brand_names TEXT,  -- JSON array
                drug_class TEXT,
                description TEXT,
                indications TEXT,
                contraindications TEXT,
                dosage TEXT,
                side_effects TEXT,
                interactions TEXT,
                article_id INTEGER,
                version TEXT DEFAULT 'home',
                language TEXT DEFAULT 'en',
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (article_id) REFERENCES articles (id)
            )
        ''')
        
        # ç–¾ç—…ç—‡çŠ¶å…³ç³»è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS disease_symptoms (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                disease_term TEXT NOT NULL,
                symptom_term TEXT NOT NULL,
                relationship_type TEXT,  -- may_have, has_symptom, causes
                confidence_score REAL DEFAULT 1.0,
                article_id INTEGER,
                version TEXT DEFAULT 'home',
                language TEXT DEFAULT 'en',
                FOREIGN KEY (article_id) REFERENCES articles (id)
            )
        ''')
        
        # æœç´¢æ—¥å¿—è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS search_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                query TEXT,
                results_count INTEGER DEFAULT 0,
                execution_time REAL DEFAULT 0.0,
                user_agent TEXT,
                ip_address TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # è´¨é‡æ£€æŸ¥è®°å½•è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS quality_checks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                article_id INTEGER,
                check_type TEXT,
                passed BOOLEAN,
                score REAL,
                issues TEXT,  -- JSON array of issues
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (article_id) REFERENCES articles (id)
            )
        ''')
        
        conn.commit()
    
    def _create_indexes(self, conn):
        """åˆ›å»ºç´¢å¼•"""
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_articles_url ON articles (url)",
            "CREATE INDEX IF NOT EXISTS idx_articles_category ON articles (category)",
            "CREATE INDEX IF NOT EXISTS idx_articles_language ON articles (language)",
            "CREATE INDEX IF NOT EXISTS idx_articles_version ON articles (version)",
            "CREATE INDEX IF NOT EXISTS idx_articles_content_hash ON articles (content_hash)",
            "CREATE INDEX IF NOT EXISTS idx_terms_term ON medical_terms (term)",
            "CREATE INDEX IF NOT EXISTS idx_terms_category ON medical_terms (category)",
            "CREATE INDEX IF NOT EXISTS idx_terms_language ON medical_terms (language)",
            "CREATE INDEX IF NOT EXISTS idx_article_terms_article_id ON article_terms (article_id)",
            "CREATE INDEX IF NOT EXISTS idx_article_terms_term_id ON article_terms (term_id)",
            "CREATE INDEX IF NOT EXISTS idx_drugs_generic_name ON drugs (generic_name)",
            "CREATE INDEX IF NOT EXISTS idx_drugs_class ON drugs (drug_class)",
            "CREATE INDEX IF NOT EXISTS idx_disease_symptoms_disease ON disease_symptoms (disease_term)",
            "CREATE INDEX IF NOT EXISTS idx_disease_symptoms_symptom ON disease_symptoms (symptom_term)",
            "CREATE INDEX IF NOT EXISTS idx_search_logs_query ON search_logs (query)",
            "CREATE INDEX IF NOT EXISTS idx_quality_checks_article_id ON quality_checks (article_id)"
        ]
        
        for index_sql in indexes:
            conn.execute(index_sql)
        
        conn.commit()
        
        # åˆ›å»ºå…¨æ–‡æœç´¢è™šæ‹Ÿè¡¨
        self._create_fts_indexes(conn)
    
    def _create_fts_indexes(self, conn):
        """åˆ›å»ºå…¨æ–‡æœç´¢ç´¢å¼•"""
        try:
            # æ–‡ç« å…¨æ–‡æœç´¢è¡¨
            conn.execute('''
                CREATE VIRTUAL TABLE IF NOT EXISTS articles_fts USING fts5(
                    title,
                    content,
                    subtitle,
                    category,
                    content='articles',
                    content_rowid='id'
                )
            ''')
            
            # åŒ»å­¦æœ¯è¯­å…¨æ–‡æœç´¢è¡¨
            conn.execute('''
                CREATE VIRTUAL TABLE IF NOT EXISTS terms_fts USING fts5(
                    term,
                    definition,
                    content='medical_terms',
                    content_rowid='id'
                )
            ''')
            
            conn.commit()
            logger.info("å…¨æ–‡æœç´¢ç´¢å¼•åˆ›å»ºæˆåŠŸ")
            
        except sqlite3.Error as e:
            logger.warning(f"åˆ›å»ºå…¨æ–‡æœç´¢ç´¢å¼•å¤±è´¥: {e}")
    
    def _create_functions(self, conn):
        """åˆ›å»ºè‡ªå®šä¹‰å‡½æ•°"""
        # æ–‡æœ¬ç›¸ä¼¼åº¦å‡½æ•°ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        def text_similarity(text1, text2):
            """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            
            if not words1 and not words2:
                return 1.0
            
            intersection = words1.intersection(words2)
            union = words1.union(words2)
            
            return len(intersection) / len(union) if union else 0.0
        
        # æ³¨å†Œå‡½æ•°
        conn.create_function("text_similarity", 2, text_similarity)
        
        # æ–‡æœ¬æ¸…ç†å‡½æ•°
        def clean_text(text):
            """æ¸…ç†æ–‡æœ¬"""
            if not text:
                return ""
            
            # ç§»é™¤å¤šä½™çš„ç©ºç™½
            import re
            text = re.sub(r'\s+', ' ', text)
            return text.strip()
        
        conn.create_function("clean_text", 1, clean_text)
    
    def insert_article(self, article_data):
        """æ’å…¥æ–‡ç« æ•°æ®"""
        conn = self.get_connection()
        try:
            cursor = conn.cursor()
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            cursor.execute("SELECT id FROM articles WHERE url = ?", (article_data['url'],))
            existing = cursor.fetchone()
            
            if existing:
                # æ›´æ–°ç°æœ‰è®°å½•
                article_id = existing[0]
                cursor.execute('''
                    UPDATE articles SET 
                        title = ?, subtitle = ?, content = ?, content_html = ?,
                        excerpt = ?, word_count = ?, quality_score = ?,
                        updated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                ''', (
                    article_data.get('title', ''),
                    article_data.get('subtitle', ''),
                    article_data.get('content', ''),
                    article_data.get('content_html', ''),
                    article_data.get('summary', ''),
                    article_data.get('word_count', 0),
                    article_data.get('quality_score', 0),
                    article_id
                ))
            else:
                # æ’å…¥æ–°è®°å½•
                cursor.execute('''
                    INSERT INTO articles (
                        url, title, subtitle, content, content_html, excerpt,
                        category, subcategory, version, language,
                        author, last_reviewed, word_count, content_hash,
                        quality_score, extracted_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    article_data['url'],
                    article_data.get('title', ''),
                    article_data.get('subtitle', ''),
                    article_data.get('content', ''),
                    article_data.get('content_html', ''),
                    article_data.get('summary', ''),
                    article_data.get('category', ''),
                    article_data.get('subcategory', ''),
                    article_data.get('version', 'home'),
                    article_data.get('language', 'en'),
                    article_data.get('metadata', {}).get('author', ''),
                    article_data.get('metadata', {}).get('last_reviewed', ''),
                    article_data.get('word_count', 0),
                    article_data.get('content_hash', ''),
                    article_data.get('quality_score', 0),
                    article_data.get('extracted_at', '')
                ))
                
                article_id = cursor.lastrowid
            
            conn.commit()
            return article_id
            
        except sqlite3.Error as e:
            conn.rollback()
            logger.error(f"æ’å…¥æ–‡ç« å¤±è´¥: {e}")
            raise
        finally:
            conn.close()
    
    def insert_medical_terms(self, article_id, medical_terms, language='zh', version='home'):
        """æ’å…¥åŒ»å­¦æœ¯è¯­"""
        conn = self.get_connection()
        try:
            cursor = conn.cursor()
            
            for term_info in medical_terms:
                term = term_info.get('term', '')
                if not term:
                    continue
                
                # æ£€æŸ¥æœ¯è¯­æ˜¯å¦å·²å­˜åœ¨
                cursor.execute(
                    "SELECT id FROM medical_terms WHERE term = ? AND language = ? AND version = ?",
                    (term, language, version)
                )
                existing_term = cursor.fetchone()
                
                if existing_term:
                    term_id = existing_term[0]
                else:
                    # åˆ›å»ºæ–°æœ¯è¯­
                    cursor.execute('''
                        INSERT INTO medical_terms (
                            term, category, language, version, related_articles
                        ) VALUES (?, ?, ?, ?, ?)
                    ''', (
                        term,
                        term_info.get('category', 'general'),
                        language,
                        version,
                        json.dumps([article_id])
                    ))
                    term_id = cursor.lastrowid
                
                # åˆ›å»ºæ–‡ç« æœ¯è¯­å…³è”
                cursor.execute('''
                    INSERT OR REPLACE INTO article_terms (
                        article_id, term_id, frequency, positions
                    ) VALUES (?, ?, ?, ?)
                ''', (
                    article_id,
                    term_id,
                    term_info.get('frequency', 1),
                    json.dumps(term_info.get('positions', []))
                ))
            
            conn.commit()
            
        except sqlite3.Error as e:
            conn.rollback()
            logger.error(f"æ’å…¥åŒ»å­¦æœ¯è¯­å¤±è´¥: {e}")
        finally:
            conn.close()
    
    def search_articles(self, query, language='zh', category=None, limit=20):
        """æœç´¢æ–‡ç« """
        conn = self.get_connection()
        try:
            cursor = conn.cursor()
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_conditions = []
            params = []
            
            if language:
                where_conditions.append("language = ?")
                params.append(language)
            
            if category:
                where_conditions.append("category = ?")
                params.append(category)
            
            where_clause = "WHERE " + " AND ".join(where_conditions) if where_conditions else ""
            
            # å…¨æ–‡æœç´¢
            cursor.execute(f'''
                SELECT a.id, a.title, a.excerpt, a.category, a.language,
                       a.word_count, a.quality_score,
                       highlight(articles_fts, 0, '<mark>', '</mark>') as highlighted_title,
                       snippet(articles_fts, 1, '<mark>', '</mark>', '...', 20) as snippet
                FROM articles a
                JOIN articles_fts ON articles_fts.rowid = a.id
                {where_clause}
                AND articles_fts MATCH ?
                ORDER BY a.quality_score DESC, a.word_count DESC
                LIMIT ?
            ''', params + [query, limit])
            
            results = []
            for row in cursor.fetchall():
                results.append({
                    'id': row[0],
                    'title': row[1],
                    'excerpt': row[2],
                    'category': row[3],
                    'language': row[4],
                    'word_count': row[5],
                    'quality_score': row[6],
                    'highlighted_title': row[7],
                    'snippet': row[8]
                })
            
            return results
            
        except sqlite3.Error as e:
            logger.error(f"æœç´¢æ–‡ç« å¤±è´¥: {e}")
            return []
        finally:
            conn.close()
    
    def get_statistics(self):
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        conn = self.get_connection()
        try:
            cursor = conn.cursor()
            
            # åŸºæœ¬ç»Ÿè®¡
            stats = {}
            
            # æ–‡ç« æ€»æ•°
            cursor.execute("SELECT COUNT(*) FROM articles")
            stats['total_articles'] = cursor.fetchone()[0]
            
            # æŒ‰è¯­è¨€ç»Ÿè®¡
            cursor.execute("SELECT language, COUNT(*) FROM articles GROUP BY language")
            stats['by_language'] = dict(cursor.fetchall())
            
            # æŒ‰ç‰ˆæœ¬ç»Ÿè®¡
            cursor.execute("SELECT version, COUNT(*) FROM articles GROUP BY version")
            stats['by_version'] = dict(cursor.fetchall())
            
            # æŒ‰åˆ†ç±»ç»Ÿè®¡
            cursor.execute("SELECT category, COUNT(*) FROM articles WHERE category != '' GROUP BY category ORDER BY COUNT(*) DESC LIMIT 10")
            stats['top_categories'] = dict(cursor.fetchall())
            
            # åŒ»å­¦æœ¯è¯­ç»Ÿè®¡
            cursor.execute("SELECT COUNT(*) FROM medical_terms")
            stats['total_medical_terms'] = cursor.fetchone()[0]
            
            # è¯ç‰©ç»Ÿè®¡
            cursor.execute("SELECT COUNT(*) FROM drugs")
            stats['total_drugs'] = cursor.fetchone()[0]
            
            # è´¨é‡è¯„åˆ†ç»Ÿè®¡
            cursor.execute("SELECT AVG(quality_score) FROM articles")
            avg_quality = cursor.fetchone()[0]
            stats['average_quality_score'] = round(avg_quality, 2) if avg_quality else 0
            
            # è¯æ•°ç»Ÿè®¡
            cursor.execute("SELECT AVG(word_count) FROM articles")
            avg_words = cursor.fetchone()[0]
            stats['average_word_count'] = round(avg_words, 0) if avg_words else 0
            
            return stats
            
        except sqlite3.Error as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
        finally:
            conn.close()
    
    def cleanup_data(self):
        """æ¸…ç†æ•°æ®"""
        conn = self.get_connection()
        try:
            cursor = conn.cursor()
            
            # åˆ é™¤é‡å¤æ–‡ç« ï¼ˆåŸºäºcontent_hashï¼‰
            cursor.execute('''
                DELETE FROM articles 
                WHERE id NOT IN (
                    SELECT MIN(id) 
                    FROM articles 
                    GROUP BY content_hash
                )
            ''')
            
            # åˆ é™¤å­¤ç«‹çš„åŒ»å­¦æœ¯è¯­
            cursor.execute('''
                DELETE FROM medical_terms 
                WHERE id NOT IN (
                    SELECT DISTINCT term_id 
                    FROM article_terms
                )
            ''')
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            cursor.execute("VACUUM")
            
            conn.commit()
            logger.info("æ•°æ®æ¸…ç†å®Œæˆ")
            
        except sqlite3.Error as e:
            conn.rollback()
            logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {e}")
        finally:
            conn.close()

class VectorProcessor:
    """å‘é‡å¤„ç†å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.word_vectors = {}
        self.term_frequencies = defaultdict(int)
    
    def process_text(self, text, language='zh'):
        """å¤„ç†æ–‡æœ¬ç”Ÿæˆå‘é‡"""
        if not text:
            return []
        
        # åˆ†è¯
        if language == 'zh':
            words = self._chinese_tokenize(text)
        else:
            words = self._english_tokenize(text)
        
        # è®¡ç®—è¯é¢‘
        word_freq = defaultdict(int)
        for word in words:
            word_freq[word] += 1
        
        # TF-IDFå‘é‡åŒ–ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        vector = []
        total_words = len(words)
        
        for word, freq in word_freq.items():
            # ç®€å•çš„TFï¼ˆè¯é¢‘ï¼‰
            tf = freq / total_words
            
            # ç®€åŒ–çš„IDFï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰
            idf = 1.0  # ç®€åŒ–å¤„ç†ï¼Œæ‰€æœ‰è¯éƒ½ä½¿ç”¨ç›¸åŒçš„IDF
            
            vector.append((word, tf * idf))
        
        return vector
    
    def _chinese_tokenize(self, text):
        """ä¸­æ–‡åˆ†è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        import re
        
        # ç®€å•çš„ä¸­æ–‡å­—ç¬¦åˆ†å‰²
        words = re.findall(r'[\u4e00-\u9fa5]{2,}', text)
        
        # ç§»é™¤åœç”¨è¯
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°'}
        words = [word for word in words if word not in stop_words and len(word) >= 2]
        
        return words
    
    def _english_tokenize(self, text):
        """è‹±æ–‡åˆ†è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        import re
        
        # ç®€å•çš„è‹±æ–‡å•è¯æå–
        words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())
        
        # ç§»é™¤åœç”¨è¯
        stop_words = {'the', 'and', 'or', 'but', 'with', 'for', 'in', 'on', 'at', 'to', 'of', 'is', 'are', 'was', 'were'}
        words = [word for word in words if word not in stop_words]
        
        return words
    
    def calculate_similarity(self, vector1, vector2):
        """è®¡ç®—å‘é‡ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰"""
        if not vector1 or not vector2:
            return 0.0
        
        # åˆ›å»ºè¯æ±‡è¡¨
        all_words = set()
        for word, _ in vector1 + vector2:
            all_words.add(word)
        
        # åˆ›å»ºå‘é‡
        vec1 = [0.0] * len(all_words)
        vec2 = [0.0] * len(all_words)
        
        word_to_idx = {word: i for i, word in enumerate(all_words)}
        
        for word, weight in vector1:
            vec1[word_to_idx[word]] = weight
        
        for word, weight in vector2:
            vec2[word_to_idx[word]] = weight
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = sum(a * a for a in vec1) ** 0.5
        magnitude2 = sum(b * b for b in vec2) ** 0.5
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
        
        return dot_product / (magnitude1 * magnitude2)

class QualityValidator:
    """æ•°æ®è´¨é‡éªŒè¯å™¨"""
    
    def __init__(self):
        self.quality_rules = {
            'title_min_length': 5,
            'content_min_length': 100,
            'word_count_min': 10,
            'max_repeated_chars': 0.3,
            'min_medical_terms': 1,
            'min_quality_score': 30
        }
    
    def validate_article(self, article_data):
        """éªŒè¯æ–‡ç« è´¨é‡"""
        issues = []
        score = 0
        
        # æ ‡é¢˜éªŒè¯
        title = article_data.get('title', '')
        if len(title) >= self.quality_rules['title_min_length']:
            score += 20
        else:
            issues.append(f"æ ‡é¢˜è¿‡çŸ­ ({len(title)} å­—ç¬¦)")
        
        # å†…å®¹éªŒè¯
        content = article_data.get('content', '')
        content_length = len(content)
        
        if content_length >= self.quality_rules['content_min_length']:
            score += 30
        else:
            issues.append(f"å†…å®¹è¿‡çŸ­ ({content_length} å­—ç¬¦)")
        
        # è¯æ•°éªŒè¯
        word_count = article_data.get('word_count', 0)
        if word_count >= self.quality_rules['word_count_min']:
            score += 20
        else:
            issues.append(f"è¯æ•°ä¸è¶³ ({word_count})")
        
        # åŒ»å­¦æœ¯è¯­éªŒè¯
        medical_terms = article_data.get('medical_terms', [])
        if len(medical_terms) >= self.quality_rules['min_medical_terms']:
            score += 15
        else:
            issues.append(f"åŒ»å­¦æœ¯è¯­ä¸è¶³ ({len(medical_terms)})")
        
        # URLéªŒè¯
        url = article_data.get('url', '')
        if url and url.startswith('http'):
            score += 10
        else:
            issues.append("URLæ ¼å¼æ— æ•ˆ")
        
        # å…ƒæ•°æ®éªŒè¯
        metadata = article_data.get('metadata', {})
        if metadata.get('category') and metadata.get('language'):
            score += 5
        else:
            issues.append("å…ƒæ•°æ®ä¸å®Œæ•´")
        
        # é‡å¤å†…å®¹æ£€æŸ¥
        if self._has_repeated_content(content):
            score -= 10
            issues.append("å­˜åœ¨é‡å¤å†…å®¹")
        
        # å¯è¯»æ€§æ£€æŸ¥
        readability_score = self._calculate_readability(content)
        if readability_score >= 50:
            score += 5
        else:
            issues.append(f"å¯è¯»æ€§è¾ƒå·® ({readability_score})")
        
        # é™åˆ¶æœ€å¤§åˆ†æ•°
        score = min(100, score)
        
        return {
            'is_valid': score >= self.quality_rules['min_quality_score'],
            'quality_score': score,
            'issues': issues,
            'readability_score': readability_score
        }
    
    def _has_repeated_content(self, content):
        """æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤å†…å®¹"""
        if not content:
            return False
        
        # ç®€å•çš„é‡å¤æ£€æŸ¥
        lines = content.split('\n')
        if len(lines) < 3:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶…è¿‡30%çš„è¡Œæ˜¯é‡å¤çš„
        line_counts = defaultdict(int)
        for line in lines:
            if len(line.strip()) > 10:  # åªæ£€æŸ¥æœ‰æ„ä¹‰çš„è¡Œ
                line_counts[line.strip()] += 1
        
        repeated_lines = sum(1 for count in line_counts.values() if count > 1)
        return (repeated_lines / len(lines)) > self.quality_rules['max_repeated_chars']
    
    def _calculate_readability(self, content):
        """è®¡ç®—å¯è¯»æ€§è¯„åˆ†"""
        if not content:
            return 0
        
        import re
        
        # ç®€å•çš„å¯è¯»æ€§è®¡ç®—
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', content)
        sentences = [s for s in sentences if s.strip()]
        
        if not sentences:
            return 0
        
        # å¹³å‡å¥å­é•¿åº¦
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        # å¥å­é•¿åº¦è¯„åˆ†ï¼ˆ30-50å­—ç¬¦ä¸ºæœ€ä½³ï¼‰
        if 30 <= avg_sentence_length <= 50:
            return 100
        elif avg_sentence_length < 30:
            return max(0, avg_sentence_length * 3)
        else:
            return max(0, 100 - (avg_sentence_length - 50) * 2)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ—„ï¸ åŒ»å­¦çŸ¥è¯†åº“æ•°æ®å¤„ç†å’Œå­˜å‚¨ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆå§‹åŒ–æ•°æ®åº“
    db = MedicalDatabase()
    
    # è´¨é‡éªŒè¯å™¨
    validator = QualityValidator()
    
    # å‘é‡å¤„ç†å™¨
    vector_processor = VectorProcessor()
    
    print("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    print("âœ… è´¨é‡éªŒè¯å™¨å‡†å¤‡å°±ç»ª")
    print("âœ… å‘é‡å¤„ç†å™¨å‡†å¤‡å°±ç»ª")
    
    # å¦‚æœæœ‰çˆ¬è™«æ•°æ®ï¼Œå°è¯•å¯¼å…¥
    if Path("data/output/crawler_results.json").exists():
        print("\\nğŸ“¥ æ­£åœ¨å¯¼å…¥çˆ¬è™«æ•°æ®...")
        
        with open("data/output/crawler_results.json", 'r', encoding='utf-8') as f:
            crawler_data = json.load(f)
        
        articles = crawler_data.get('articles', [])
        print(f"å‘ç° {len(articles)} ç¯‡çˆ¬å–çš„æ–‡ç« ")
        
        imported_count = 0
        for article_data in articles:
            try:
                # è´¨é‡éªŒè¯
                validation = validator.validate_article(article_data)
                article_data['quality_score'] = validation['quality_score']
                
                # å‘é‡å¤„ç†
                if article_data.get('content'):
                    content_vector = vector_processor.process_text(
                        article_data['content'], 
                        article_data.get('language', 'zh')
                    )
                    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šå°†å‘é‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                
                # æ’å…¥æ•°æ®åº“
                article_id = db.insert_article(article_data)
                
                # æ’å…¥åŒ»å­¦æœ¯è¯­
                medical_terms = article_data.get('medical_terms', [])
                if medical_terms:
                    db.insert_medical_terms(
                        article_id, 
                        medical_terms,
                        article_data.get('language', 'zh'),
                        article_data.get('version', 'home')
                    )
                
                imported_count += 1
                
            except Exception as e:
                logger.error(f"å¯¼å…¥æ–‡ç« å¤±è´¥: {e}")
        
        print(f"âœ… æˆåŠŸå¯¼å…¥ {imported_count} ç¯‡æ–‡ç« ")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
    stats = db.get_statistics()
    
    for key, value in stats.items():
        if key == 'by_language':
            print(f"  æŒ‰è¯­è¨€åˆ†å¸ƒ: {value}")
        elif key == 'by_version':
            print(f"  æŒ‰ç‰ˆæœ¬åˆ†å¸ƒ: {value}")
        elif key == 'top_categories':
            print(f"  çƒ­é—¨åˆ†ç±»: {dict(list(value.items())[:5])}")
        elif key == 'average_quality_score':
            print(f"  å¹³å‡è´¨é‡è¯„åˆ†: {value}")
        elif key == 'average_word_count':
            print(f"  å¹³å‡è¯æ•°: {int(value)}")
        elif isinstance(value, int):
            print(f"  {key}: {value:,}")
    
    # æµ‹è¯•æœç´¢åŠŸèƒ½
    if stats.get('total_articles', 0) > 0:
        print("\\nğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½:")
        test_queries = ["é«˜è¡€å‹", "heart", "diabetes"]
        
        for query in test_queries:
            results = db.search_articles(query, limit=5)
            print(f"  æŸ¥è¯¢ '{query}': æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            
            if results:
                for result in results[:3]:  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
                    print(f"    - {result['title']} (è´¨é‡: {result['quality_score']})")
    
    print("\\nğŸ‰ æ•°æ®å¤„ç†å’Œå­˜å‚¨ç³»ç»Ÿè®¾ç½®å®Œæˆï¼")

if __name__ == "__main__":
    main()
