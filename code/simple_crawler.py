#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆé»˜æ²™ä¸œè¯Šç–—æ‰‹å†Œçˆ¬è™«ï¼ˆä¸ä¾èµ–å¤–éƒ¨åº“ï¼‰
"""

import os
import sys
import json
import re
import hashlib
import logging
import requests
from urllib.parse import urljoin, urlparse
from datetime import datetime
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleContentParser:
    """ç®€åŒ–ç‰ˆå†…å®¹è§£æå™¨"""
    
    def __init__(self):
        self.stop_words = {
            'zh': ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°'],
            'en': ['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'it', 'for', 'not']
        }
    
    def extract_title(self, html_content, url):
        """æå–æ ‡é¢˜"""
        # ç®€å•çš„æ ‡é¢˜æå–
        title_match = re.search(r'<title[^>]*>([^<]+)</title>', html_content, re.IGNORECASE)
        if title_match:
            title = title_match.group(1).strip()
            # æ¸…ç†æ ‡é¢˜
            title = re.sub(r'\s*\|.*MSD.*Manuals\s*', '', title)
            title = re.sub(r'\s*\|.*ä¸“ä¸šç‰ˆ.*\s*', '', title)
            title = title.strip()
            return title if len(title) > 3 else ""
        
        # å°è¯•ä»h1æå–
        h1_match = re.search(r'<h1[^>]*>([^<]+)</h1>', html_content, re.IGNORECASE)
        if h1_match:
            return h1_match.group(1).strip()
        
        return "æœªå‘½åé¡µé¢"
    
    def extract_content(self, html_content):
        """æå–æ­£æ–‡å†…å®¹"""
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
        content = re.sub(r'<style[^>]*>.*?</style>', '', content, flags=re.DOTALL | re.IGNORECASE)
        
        # ç§»é™¤æ³¨é‡Š
        content = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)
        
        # å°è¯•æå–ä¸»è¦å†…å®¹åŒºåŸŸ
        main_patterns = [
            r'<main[^>]*>(.*?)</main>',
            r'<article[^>]*>(.*?)</article>',
            r'<div[^>]*class="[^"]*content[^"]*"[^>]*>(.*?)</div>',
            r'<div[^>]*class="[^"]*article[^"]*"[^>]*>(.*?)</div>'
        ]
        
        for pattern in main_patterns:
            match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
            if match:
                content = match.group(1)
                break
        
        # ç§»é™¤HTMLæ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', content)
        
        # è§£ç HTMLå®ä½“
        html_entities = {
            '&nbsp;': ' ', '&amp;': '&', '&lt;': '<', '&gt;': '>',
            '&quot;': '"', '&#39;': "'", '&mdash;': 'â€”', '&ndash;': 'â€“'
        }
        for entity, char in html_entities.items():
            content = content.replace(entity, char)
        
        # æ¸…ç†ç©ºç™½
        content = re.sub(r'\s+', ' ', content)
        content = content.strip()
        
        return content
    
    def extract_metadata(self, html_content, url):
        """æå–å…ƒæ•°æ®"""
        metadata = {
            'url': url,
            'language': 'en',
            'category': '',
            'author': '',
            'version': self._determine_version(url)
        }
        
        # ä»metaæ ‡ç­¾æå–
        meta_patterns = [
            (r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']', 'description'),
            (r'<meta[^>]*name=["\']author["\'][^>]*content=["\']([^"\']*)["\']', 'author'),
            (r'<meta[^>]*name=["\']language["\'][^>]*content=["\']([^"\']*)["\']', 'language')
        ]
        
        for pattern, key in meta_patterns:
            match = re.search(pattern, html_content, re.IGNORECASE)
            if match:
                metadata[key] = match.group(1).strip()
        
        # ä»URLæå–åˆ†ç±»
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.split('/')
        if len(path_parts) > 2:
            category_part = path_parts[2] if len(path_parts) > 2 else ''
            metadata['category'] = category_part
        
        return metadata
    
    def extract_links(self, html_content, base_url):
        """æå–é“¾æ¥"""
        links = []
        link_pattern = r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>([^<]*)</a>'
        
        for match in re.finditer(link_pattern, html_content, re.IGNORECASE):
            href = match.group(1)
            text = match.group(2).strip()
            
            if href and text:
                # æ„å»ºå®Œæ•´URL
                full_url = urljoin(base_url, href)
                links.append({
                    'url': full_url,
                    'text': text
                })
        
        return links
    
    def extract_medical_terms(self, content, language='zh'):
        """æå–åŒ»å­¦æœ¯è¯­"""
        terms = []
        
        # ä¸­æ–‡åŒ»å­¦æœ¯è¯­
        if language == 'zh':
            medical_patterns = [
                'é«˜è¡€å‹', 'ä½è¡€å‹', 'å¿ƒè„ç—…', 'ç³–å°¿ç—…', 'ç™Œç—‡', 'è‚¿ç˜¤', 'ç‚ç—‡', 'æ„ŸæŸ“',
                'ç—‡çŠ¶', 'è¯Šæ–­', 'æ²»ç–—', 'é¢„é˜²', 'è¯ç‰©', 'æ‰‹æœ¯', 'æ£€æŸ¥',
                'åŒ»ç”Ÿ', 'æ‚£è€…', 'åŒ»é™¢', 'æ€¥æ•‘', 'æŠ¤ç†', 'åº·å¤'
            ]
            
            for pattern in medical_patterns:
                if pattern in content:
                    terms.append({
                        'term': pattern,
                        'context': self._get_context(content, pattern),
                        'frequency': content.count(pattern)
                    })
        else:
            # è‹±æ–‡åŒ»å­¦æœ¯è¯­
            medical_patterns = [
                'disease', 'disorder', 'syndrome', 'diagnosis', 'treatment',
                'symptom', 'medication', 'surgery', 'examination', 'therapy',
                'pathology', 'physiology', 'anatomy', 'cardiology', 'neurology'
            ]
            
            for pattern in medical_patterns:
                if re.search(r'\b' + re.escape(pattern) + r'\b', content, re.IGNORECASE):
                    terms.append({
                        'term': pattern,
                        'context': self._get_context(content, pattern),
                        'frequency': len(re.findall(r'\b' + re.escape(pattern) + r'\b', content, re.IGNORECASE))
                    })
        
        return terms
    
    def _get_context(self, content, term):
        """è·å–æœ¯è¯­ä¸Šä¸‹æ–‡"""
        # ç®€å•çš„ä¸Šä¸‹æ–‡æå–
        term_pos = content.find(term)
        if term_pos == -1:
            return ""
        
        start = max(0, term_pos - 50)
        end = min(len(content), term_pos + len(term) + 50)
        
        return content[start:end].strip()
    
    def _determine_version(self, url):
        """ç¡®å®šç‰ˆæœ¬"""
        if '/home/' in url:
            return 'home'
        elif '/professional/' in url:
            return 'professional'
        elif 'msdvetmanual.com' in url:
            return 'veterinary'
        return 'home'
    
    def parse(self, url, html_content):
        """è§£æé¡µé¢"""
        try:
            # æå–å„ä¸ªéƒ¨åˆ†
            title = self.extract_title(html_content, url)
            content = self.extract_content(html_content)
            metadata = self.extract_metadata(html_content, url)
            links = self.extract_links(html_content, url)
            medical_terms = self.extract_medical_terms(content, metadata.get('language', 'zh'))
            
            # æ„å»ºç»“æœ
            result = {
                'url': url,
                'title': title,
                'content': content,
                'content_length': len(content),
                'word_count': len(content.split()),
                'metadata': metadata,
                'links': links,
                'medical_terms': medical_terms,
                'extracted_at': datetime.utcnow().isoformat(),
                'content_hash': hashlib.sha256(content.encode('utf-8')).hexdigest()
            }
            
            return result
            
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥ {url}: {e}")
            raise

class SimpleDataProcessor:
    """ç®€åŒ–ç‰ˆæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        self.quality_thresholds = {
            'min_content_length': 50,
            'min_word_count': 10,
            'min_title_length': 3
        }
    
    def process(self, parsed_data):
        """å¤„ç†æ•°æ®"""
        try:
            # æ•°æ®æ¸…æ´—
            cleaned_data = self._clean_data(parsed_data)
            
            # è´¨é‡è¯„ä¼°
            quality_assessed_data = self._assess_quality(cleaned_data)
            
            # ç‰¹å¾æå–
            feature_data = self._extract_features(quality_assessed_data)
            
            return feature_data
            
        except Exception as e:
            logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
            raise
    
    def _clean_data(self, data):
        """æ¸…æ´—æ•°æ®"""
        cleaned = data.copy()
        
        # æ¸…ç†æ ‡é¢˜
        if cleaned.get('title'):
            cleaned['title'] = self._clean_title(cleaned['title'])
        
        # æ¸…ç†å†…å®¹
        if cleaned.get('content'):
            cleaned['content'] = self._clean_content(cleaned['content'])
        
        return cleaned
    
    def _clean_title(self, title):
        """æ¸…ç†æ ‡é¢˜"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½
        title = re.sub(r'\s+', ' ', title.strip())
        # ç§»é™¤å•†æ ‡ä¿¡æ¯
        title = re.sub(r'\|.*MSD.*Manuals', '', title, flags=re.IGNORECASE)
        return title.strip()
    
    def _clean_content(self, content):
        """æ¸…ç†å†…å®¹"""
        # ç§»é™¤é‡å¤çš„æ¢è¡Œ
        content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)
        # ç§»é™¤è¿‡é•¿çš„ç©ºç™½
        content = re.sub(r'\s{5,}', ' ', content)
        return content.strip()
    
    def _assess_quality(self, data):
        """è¯„ä¼°è´¨é‡"""
        quality_score = 0
        issues = []
        
        # æ ‡é¢˜è´¨é‡æ£€æŸ¥
        title = data.get('title', '')
        if len(title) >= self.quality_thresholds['min_title_length']:
            quality_score += 25
        else:
            issues.append("æ ‡é¢˜è¿‡çŸ­")
        
        # å†…å®¹è´¨é‡æ£€æŸ¥
        content = data.get('content', '')
        word_count = data.get('word_count', 0)
        if len(content) >= self.quality_thresholds['min_content_length']:
            quality_score += 25
        else:
            issues.append("å†…å®¹è¿‡çŸ­")
        
        if word_count >= self.quality_thresholds['min_word_count']:
            quality_score += 25
        else:
            issues.append("è¯æ•°ä¸è¶³")
        
        # åŒ»å­¦æœ¯è¯­è´¨é‡
        medical_terms = data.get('medical_terms', [])
        if len(medical_terms) >= 3:
            quality_score += 25
        else:
            issues.append("åŒ»å­¦æœ¯è¯­ä¸è¶³")
        
        data['quality_score'] = quality_score
        data['quality_issues'] = issues
        
        return data
    
    def _extract_features(self, data):
        """æå–ç‰¹å¾"""
        # è®¡ç®—å…³é”®è¯ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        content = data.get('content', '')
        language = data.get('metadata', {}).get('language', 'zh')
        
        # ç®€å•çš„å…³é”®è¯æå–
        if language == 'zh':
            # ä¸­æ–‡å­—ç¬¦åˆ†è¯
            import jieba
            words = jieba.lcut(content)
            # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
            stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'æœ‰', 'ä¸', 'ä¸€', 'ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦'}
            keywords = [word for word in words if len(word) >= 2 and word not in stop_words]
        else:
            # è‹±æ–‡å•è¯
            words = re.findall(r'\b[a-zA-Z]{3,}\b', content.lower())
            stop_words = {'the', 'and', 'or', 'but', 'with', 'for', 'in', 'on', 'at', 'to', 'of'}
            keywords = [word for word in words if word not in stop_words]
        
        # ç»Ÿè®¡é¢‘ç‡
        from collections import Counter
        word_freq = Counter(keywords)
        top_keywords = word_freq.most_common(10)
        
        data['keywords'] = [{'keyword': word, 'frequency': freq} for word, freq in top_keywords]
        
        # ç”Ÿæˆæ‘˜è¦
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        if sentences:
            data['summary'] = ' '.join(sentences[:3])
        else:
            data['summary'] = content[:200] + '...' if len(content) > 200 else content
        
        return data

class SimpleMSDCrawler:
    """ç®€åŒ–ç‰ˆMSDæ‰‹å†Œçˆ¬è™«"""
    
    def __init__(self):
        self.parser = SimpleContentParser()
        self.processor = SimpleDataProcessor()
        self.seen_urls = set()
        self.processed_count = 0
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs('data/output', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
    
    def is_allowed_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦å…è®¸çˆ¬å–"""
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()
        
        # å…è®¸çš„åŸŸå
        allowed_domains = [
            'www.msdmanuals.com',
            'www.msdmanuals.cn',
            'www.msdvetmanual.com'
        ]
        
        if domain not in allowed_domains:
            return False
        
        # ç¦æ­¢çš„è·¯å¾„
        disallowed_paths = [
            '/sitecore/', '/custom/', '/news/external/',
            '/multimedia/zk/', '/downloadtextfile'
        ]
        
        for path in disallowed_paths:
            if path in parsed_url.path.lower():
                return False
        
        return True
    
    def crawl_url(self, url, delay=5):
        """çˆ¬å–å•ä¸ªURL"""
        try:
            logger.info(f"æ­£åœ¨çˆ¬å–: {url}")
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            # å‘é€è¯·æ±‚
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # è§£æå†…å®¹
            parsed_data = self.parser.parse(url, response.text)
            
            # å¤„ç†æ•°æ®
            processed_data = self.processor.process(parsed_data)
            
            # ä¿å­˜æ•°æ®
            self._save_data(processed_data)
            
            # å»¶è¿Ÿ
            import time
            time.sleep(delay)
            
            self.processed_count += 1
            logger.info(f"âœ… æˆåŠŸå¤„ç†: {url} (å…± {self.processed_count} ä¸ª)")
            
            return processed_data
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å¤±è´¥ {url}: {e}")
            return None
    
    def _save_data(self, data):
        """ä¿å­˜æ•°æ®"""
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            url_hash = hashlib.md5(data['url'].encode('utf-8')).hexdigest()[:8]
            filename = f"data/output/article_{url_hash}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“„ æ•°æ®å·²ä¿å­˜: {filename}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def discover_urls(self, base_url, html_content):
        """å‘ç°æ–°URL"""
        new_urls = []
        
        # æå–é“¾æ¥
        links = self.parser.extract_links(html_content, base_url)
        
        for link in links:
            url = link['url']
            text = link.get('text', '').lower()
            
            # æ£€æŸ¥URLè´¨é‡
            if not self.is_allowed_url(url):
                continue
            
            if url in self.seen_urls:
                continue
            
            # è¿‡æ»¤ä½è´¨é‡é“¾æ¥
            skip_patterns = [
                'mailto:', 'tel:', 'javascript:', '#',
                'login', 'register', 'search?', 'subscribe'
            ]
            
            if any(pattern in url.lower() for pattern in skip_patterns):
                continue
            
            # ä¼˜å…ˆåŒ»å­¦ç›¸å…³é“¾æ¥
            medical_keywords = [
                'health', 'medical', 'disease', 'disorder', 'symptom',
                'treatment', 'diagnosis', 'health-topics', 'cardiovascular',
                'ç¥ç»', 'å¿ƒè„', 'ç–¾ç—…', 'ç—‡çŠ¶', 'æ²»ç–—', 'è¯Šæ–­'
            ]
            
            is_medical = any(keyword in text or keyword in url.lower() for keyword in medical_keywords)
            
            new_urls.append({
                'url': url,
                'priority': 1 if is_medical else 2,
                'source_url': base_url
            })
            
            self.seen_urls.add(url)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        new_urls.sort(key=lambda x: x['priority'])
        
        return new_urls
    
    def run(self, start_urls, max_pages=10):
        """è¿è¡Œçˆ¬è™«"""
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬è™«ä»»åŠ¡: {len(start_urls)} ä¸ªèµ·å§‹URLï¼Œæœ€å¤šå¤„ç† {max_pages} é¡µ")
        
        url_queue = []
        for url in start_urls:
            url_queue.append({'url': url, 'priority': 1, 'source_url': None})
        
        processed_data = []
        
        try:
            while url_queue and self.processed_count < max_pages:
                current_url_info = url_queue.pop(0)
                current_url = current_url_info['url']
                
                # çˆ¬å–å½“å‰URL
                result = self.crawl_url(current_url, delay=5)
                
                if result:
                    processed_data.append(result)
                    
                    # å‘ç°æ–°URLï¼ˆéœ€è¦è·å–HTMLå†…å®¹ï¼‰
                    try:
                        response = requests.get(current_url, timeout=30)
                        if response.status_code == 200:
                            new_urls = self.discover_urls(current_url, response.text)
                            url_queue.extend(new_urls)
                    except:
                        pass  # å¿½ç•¥URLå‘ç°é”™è¯¯
                
                # å®šæœŸä¿å­˜è¿›åº¦
                if self.processed_count % 5 == 0:
                    self._save_progress()
        
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¿›åº¦...")
        
        except Exception as e:
            logger.error(f"çˆ¬è™«è¿è¡Œé”™è¯¯: {e}")
        
        finally:
            # ä¿å­˜æœ€ç»ˆç»“æœ
            self._save_final_results(processed_data)
            
            logger.info(f"ğŸ‰ çˆ¬è™«å®Œæˆ! æ€»è®¡å¤„ç† {self.processed_count} ä¸ªé¡µé¢")
            return processed_data
    
    def _save_progress(self):
        """ä¿å­˜è¿›åº¦"""
        progress = {
            'processed_count': self.processed_count,
            'seen_urls_count': len(self.seen_urls),
            'timestamp': datetime.utcnow().isoformat()
        }
        
        with open('logs/crawler_progress.json', 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    
    def _save_final_results(self, data_list):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        # ä¿å­˜æ‰€æœ‰æ•°æ®
        final_data = {
            'summary': {
                'total_pages': len(data_list),
                'crawled_at': datetime.utcnow().isoformat(),
                'quality_scores': [d.get('quality_score', 0) for d in data_list]
            },
            'articles': data_list
        }
        
        with open('data/output/crawler_results.json', 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°: data/output/crawler_results.json")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” ç®€åŒ–ç‰ˆé»˜æ²™ä¸œè¯Šç–—æ‰‹å†Œçˆ¬è™«")
    print("=" * 50)
    
    # åˆ›å»ºçˆ¬è™«
    crawler = SimpleMSDCrawler()
    
    # æµ‹è¯•URLs
    test_urls = [
        'https://www.msdmanuals.com/home/',
        'https://www.msdmanuals.com/home/health-topics/'
    ]
    
    try:
        # è¿è¡Œçˆ¬è™«ï¼ˆåªå¤„ç†2é¡µè¿›è¡Œæ¼”ç¤ºï¼‰
        results = crawler.run(test_urls, max_pages=2)
        
        print(f"\\nğŸ“Š çˆ¬å–ç»“æœæ±‡æ€»:")
        print(f"- å¤„ç†é¡µé¢æ•°: {len(results)}")
        
        if results:
            print(f"- å¹³å‡è´¨é‡è¯„åˆ†: {sum(d.get('quality_score', 0) for d in results) / len(results):.1f}")
            print(f"- æ€»åŒ»å­¦æœ¯è¯­æ•°: {sum(len(d.get('medical_terms', [])) for d in results)}")
            print(f"- æ€»è¯æ•°: {sum(d.get('word_count', 0) for d in results)}")
        
        print(f"\\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"- è¯¦ç»†æ•°æ®: data/output/")
        print(f"- æ±‡æ€»ç»“æœ: data/output/crawler_results.json")
        print(f"- è¿›åº¦æ—¥å¿—: logs/crawler_progress.json")
        
    except Exception as e:
        logger.error(f"çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
