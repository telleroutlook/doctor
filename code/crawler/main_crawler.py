#!/usr/bin/env python3
"""
é»˜æ²™ä¸œè¯Šç–—æ‰‹å†Œä¸»çˆ¬è™«ç±»
"""

import os
import sys
import json
import time
import hashlib
import logging
import requests
import random
from urllib.parse import urljoin, urlparse
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from config.crawler_config import *
from database.setup_database import DatabaseManager
from database.models import Article as ArticleModel
from parsers.content_parser import MSDContentParser
from processors.data_processor import DataProcessor

logger = logging.getLogger(__name__)

class CrawlerState:
    """çˆ¬è™«çŠ¶æ€ç®¡ç†"""

    def __init__(self, state_file="crawler_state.json"):
        self.state_file = state_file
        self.state = self._default_state()
        self.load_state()

    def _default_state(self):
        """ç”Ÿæˆé»˜è®¤çŠ¶æ€ç»“æ„"""
        return {
            "last_saved": None,
            "urls_processed": 0,
            "successful_downloads": 0,
            "landing_pages_skipped": 0,
            "failed_downloads": 0,
            "parse_errors": 0,
            "duplicates_found": 0,
            "new_articles_created": 0,
            "existing_articles_updated": 0,
            "processing_time": 0.0,
            "current_url": None,
            "error_log": [],
            "processed_urls": set(),
            "failed_urls": set(),
            "checkpoint_urls": []
        }
    
    def save_state(self):
        """ä¿å­˜çˆ¬è™«çŠ¶æ€"""
        try:
            self.state["last_saved"] = datetime.now().isoformat()
            # å°†setè½¬æ¢ä¸ºlistç”¨äºJSONåºåˆ—åŒ–
            state_copy = self.state.copy()
            state_copy["processed_urls"] = list(self.state["processed_urls"])
            state_copy["failed_urls"] = list(self.state["failed_urls"])

            state_dir = os.path.dirname(self.state_file)
            if state_dir:
                os.makedirs(state_dir, exist_ok=True)

            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state_copy, f, ensure_ascii=False, indent=2)

            logger.info(f"çŠ¶æ€å·²ä¿å­˜: {self.state_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")
    
    def load_state(self):
        """åŠ è½½çˆ¬è™«çŠ¶æ€"""
        try:
            if os.path.exists(self.state_file):
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    saved_state = json.load(f)
                
                # æ¢å¤setç±»å‹
                self.state.update(saved_state)
                self.state["processed_urls"] = set(self.state["processed_urls"])
                self.state["failed_urls"] = set(self.state["failed_urls"])

                logger.info(f"å·²åŠ è½½çŠ¶æ€: å·²å¤„ç† {len(self.state['processed_urls'])} ä¸ªURL")
                
        except Exception as e:
            logger.warning(f"åŠ è½½çŠ¶æ€å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤çŠ¶æ€")
    
    def update_stats(self, **kwargs):
        """æ›´æ–°ç»Ÿè®¡æ•°æ®"""
        for key, value in kwargs.items():
            if key in self.state:
                self.state[key] += value
    
    def add_processed_url(self, url):
        """æ·»åŠ å·²å¤„ç†çš„URL"""
        self.state["processed_urls"].add(url)
        self.state["urls_processed"] = len(self.state["processed_urls"])
    
    def add_failed_url(self, url, error_msg=""):
        """æ·»åŠ å¤±è´¥çš„URL"""
        self.state["failed_urls"].add(url)
        self.state["error_log"].append({
            "url": url,
            "error": error_msg,
            "timestamp": datetime.now().isoformat()
        })

    def reset(self):
        """é‡ç½®çŠ¶æ€ï¼Œä¾¿äºé‡æ–°å¼€å§‹çˆ¬å–"""
        self.state = self._default_state()
        if os.path.exists(self.state_file):
            try:
                os.remove(self.state_file)
            except OSError as exc:
                logger.warning(f"åˆ é™¤æ—§çŠ¶æ€æ–‡ä»¶å¤±è´¥: {exc}")
        logger.info("çˆ¬è™«çŠ¶æ€å·²é‡ç½®")

class MSDManualsCrawler:
    """é»˜æ²™ä¸œè¯Šç–—æ‰‹å†Œçˆ¬è™«"""
    
    def __init__(self, config_path=None):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.config = self._load_config(config_path)
        self.db_manager = DatabaseManager(use_sqlite=True)
        self.content_parser = MSDContentParser()
        self.data_processor = DataProcessor()

        # çŠ¶æ€ç®¡ç†
        state_config = self.config.get('state', {})
        self.state_manager = CrawlerState(state_file=state_config.get('state_file', 'crawler_state.json'))

        # æ”¯æŒçš„è¯­è¨€-ç‰ˆæœ¬é…ç½®
        self.language_versions = self.config.get('language_versions', {})
        
        # è¯·æ±‚ä¼šè¯
        self.session = requests.Session()
        self._setup_session()
        
        # URLé˜Ÿåˆ—å’Œå»é‡
        self.url_queue = []
        self.seen_urls = set()
        
        # æ€§èƒ½ç›‘æ§
        self.start_time = None
        self.stats = defaultdict(int)
        self._landing_page_skip_logged = False
        
        logger.info("çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if config_path and os.path.exists(config_path):
            # åŠ¨æ€å¯¼å…¥é…ç½®æ–‡ä»¶
            import importlib.util
            spec = importlib.util.spec_from_file_location("config", config_path)
            config_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(config_module)

            return {
                'crawler': config_module.CRAWLER_CONFIG,
                'user_agents': config_module.USER_AGENTS,
                'headers': config_module.DEFAULT_HEADERS,
                'domain_configs': config_module.DOMAIN_CONFIGS,
                'retry': config_module.RETRY_CONFIG,
                'state': config_module.STATE_CONFIG,
                'language_versions': getattr(config_module, 'LANGUAGE_VERSION_URLS', {})
            }
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            return {
                'crawler': CRAWLER_CONFIG,
                'user_agents': USER_AGENTS,
                'headers': DEFAULT_HEADERS,
                'domain_configs': DOMAIN_CONFIGS,
                'retry': RETRY_CONFIG,
                'state': STATE_CONFIG,
                'language_versions': LANGUAGE_VERSION_URLS
            }
    
    def _setup_session(self):
        """è®¾ç½®è¯·æ±‚ä¼šè¯"""
        # è®¾ç½®é»˜è®¤å¤´éƒ¨
        self.session.headers.update(self.config['headers'])
        
        # è®¾ç½®Cookieå¤„ç†
        self.session.cookies.clear()
    
    def _get_random_user_agent(self):
        """è·å–éšæœºç”¨æˆ·ä»£ç†"""
        return random.choice(self.config['user_agents'])
    
    def _get_domain_delay(self, url):
        """è·å–åŸŸåç‰¹å®šçš„å»¶è¿Ÿæ—¶é—´"""
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()
        
        # é»˜è®¤å»¶è¿Ÿ
        delay = self.config['crawler']['delay_between_requests']
        
        # åŸŸåç‰¹å®šé…ç½®
        if domain in self.config['domain_configs']:
            delay = self.config['domain_configs'][domain].get('delay', delay)
        
        # æ·»åŠ éšæœºåŒ–
        if self.config['crawler']['randomize_download_delay']:
            import random
            min_delay, max_delay = self.config['crawler']['download_delay_range']
            delay = random.uniform(min_delay, max_delay)
        
        return delay
    
    def _is_allowed_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦å…è®¸çˆ¬å–"""
        parsed_url = urlparse(url)
        
        # æ£€æŸ¥åŸŸå
        domain = parsed_url.netloc.lower()
        allowed_domains = [cfg.lower() for cfg in self.config['domain_configs'].keys()]
        
        if domain not in allowed_domains:
            return False
        
        # æ£€æŸ¥è·¯å¾„
        path = parsed_url.path.lower()
        disallowed_paths = [
            '/sitecore/',
            '/custom/',
            '/news/external/',
            '/multimedia/zk/',
            '/downloadtextfile',
            '/pagerevalidation',
            '/bigqueryexport'
        ]
        
        for disallowed in disallowed_paths:
            if disallowed in path:
                return False
        
        return True
    
    def _download_page(self, url):
        """ä¸‹è½½ç½‘é¡µ"""
        try:
            # è®¾ç½®ç”¨æˆ·ä»£ç†
            self.session.headers['User-Agent'] = self._get_random_user_agent()
            
            # è·å–å»¶è¿Ÿæ—¶é—´
            delay = self._get_domain_delay(url)
            
            logger.debug(f"æ­£åœ¨ä¸‹è½½: {url}")
            
            # å‘é€è¯·æ±‚
            response = self.session.get(
                url,
                timeout=self.config['crawler']['timeout'],
                allow_redirects=True
            )
            
            # æ£€æŸ¥å“åº”
            response.raise_for_status()
            
            # å»¶è¿Ÿ
            time.sleep(delay)
            
            return response
            
        except requests.exceptions.RequestException as e:
            logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            raise
    
    def _parse_page(self, response):
        """è§£æé¡µé¢å†…å®¹"""
        try:
            # ä½¿ç”¨å†…å®¹è§£æå™¨è§£æ
            parsed_data = self.content_parser.parse(response)

            if parsed_data is None:
                self.stats['landing_pages_skipped'] += 1
                if not self._landing_page_skip_logged:
                    logger.info(
                        "è·³è¿‡ç–‘ä¼¼å¯¼èˆª/æ¦‚è§ˆé¡µ: %s (æœ¬æ¬¡è¿è¡Œåªè®°å½•é¦–æ¬¡è·³è¿‡äº‹ä»¶)", response.url
                    )
                    self._landing_page_skip_logged = True
                return None
            
            # å¤„ç†å’ŒéªŒè¯æ•°æ®
            processed_data = self.data_processor.process(parsed_data)
            
            return processed_data
            
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥ {response.url}: {e}")
            raise
    
    def _save_to_database(self, data):
        """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“"""
        session = self.db_manager.get_session()

        try:
            existing_article = session.query(ArticleModel).filter_by(url=data['url']).first()

            content_hash = hashlib.sha256(data.get('content', '').encode('utf-8')).hexdigest()

            if existing_article:
                existing_article.title = data.get('title', existing_article.title)
                existing_article.content = data.get('content', existing_article.content)
                existing_article.category = data.get('category', existing_article.category)
                existing_article.subcategory = data.get('subcategory', existing_article.subcategory)
                existing_article.language = data.get('language', existing_article.language)
                existing_article.version = data.get('version', existing_article.version)
                existing_article.hash_content = content_hash
                existing_article.word_count = len(data.get('content', '').split()) if data.get('content') else existing_article.word_count
                existing_article.updated_at = datetime.utcnow()
                self.state_manager.update_stats(existing_articles_updated=1)
            else:
                article = ArticleModel(
                    url=data['url'],
                    title=data.get('title', ''),
                    content=data.get('content', ''),
                    category=data.get('category', ''),
                    subcategory=data.get('subcategory', ''),
                    version=data.get('version', 'home'),
                    language=data.get('language', 'en'),
                    hash_content=content_hash,
                    word_count=len(data.get('content', '').split()) if data.get('content') else 0
                )
                session.add(article)
                self.state_manager.update_stats(new_articles_created=1)

            session.commit()

        except Exception as e:
            session.rollback()
            logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            raise
        finally:
            session.close()
    
    def discover_urls(self, response):
        """å‘ç°æ–°çš„URL"""
        parsed_data = self.content_parser.extract_links(response)
        
        urls = []
        for link_data in parsed_data.get('links', []):
            url = link_data.get('url')
            text = link_data.get('text', '')
            
            if url and self._is_allowed_url(url):
                # æ„å»ºå®Œæ•´URL
                full_url = urljoin(response.url, url)
                
                if full_url not in self.seen_urls and full_url not in self.state_manager.state['processed_urls']:
                    urls.append({
                        'url': full_url,
                        'text': text,
                        'source_url': response.url
                    })
                    self.seen_urls.add(full_url)
        
        return urls
    
    def get_language_version_pairs(self, language=None, version=None):
        """è·å–æ”¯æŒçš„è¯­è¨€-ç‰ˆæœ¬ç»„åˆ"""
        pairs = []

        for version_name, languages in self.language_versions.items():
            if version and version not in ('all', version_name):
                continue

            for lang_code in languages.keys():
                if language and language not in ('all', lang_code):
                    continue
                pairs.append((lang_code, version_name))

        return pairs

    def run(self, language='en', version='home', max_pages=1000, output_dir=None, reset_state=False):
        """è¿è¡Œçˆ¬è™«"""
        logger.info(f"å¼€å§‹çˆ¬å–: è¯­è¨€={language}, ç‰ˆæœ¬={version}, æœ€å¤§é¡µé¢={max_pages}")

        self.start_time = time.time()

        if reset_state:
            self.state_manager.reset()

        # æ¸…ç†é˜Ÿåˆ—å’Œå·²è§URLï¼Œç¡®ä¿å¤šæ¬¡è¿è¡Œäº’ä¸å½±å“
        self.url_queue = []
        self.seen_urls.clear()

        # åˆå§‹åŒ–ç›®æ ‡URLs
        self._initialize_urls(language, version)
        
        pages_crawled = 0
        
        try:
            while self.url_queue and pages_crawled < max_pages:
                current_url_info = self.url_queue.pop(0)
                current_url = current_url_info['url']
                
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                if current_url in self.state_manager.state['processed_urls']:
                    continue
                
                try:
                    logger.info(f"æ­£åœ¨å¤„ç† ({pages_crawled + 1}/{max_pages}): {current_url}")
                    
                    # ä¸‹è½½é¡µé¢
                    response = self._download_page(current_url)
                    
                    # è§£æå†…å®¹
                    parsed_data = self._parse_page(response)
                    landing_skipped = parsed_data is None

                    # ä¿å­˜æ•°æ®ï¼ˆå¯¼èˆª/æ¦‚è§ˆé¡µä¸ä¿å­˜ï¼‰
                    if not landing_skipped:
                        self._save_to_database(parsed_data)

                    # å‘ç°æ–°URLs
                    new_urls = self.discover_urls(response)
                    self.url_queue.extend(new_urls)

                    # æ›´æ–°ç»Ÿè®¡
                    stats_update = {'urls_processed': 1}
                    if landing_skipped:
                        stats_update['landing_pages_skipped'] = 1
                    else:
                        stats_update['successful_downloads'] = 1
                    self.state_manager.update_stats(**stats_update)

                    pages_crawled += 1
                    
                    # å®šæœŸä¿å­˜çŠ¶æ€
                    if pages_crawled % self.config['state']['save_interval'] == 0:
                        self.state_manager.save_state()
                        logger.info(f"å·²ä¿å­˜çŠ¶æ€ï¼Œå·²å¤„ç† {pages_crawled} é¡µ")
                    
                except Exception as e:
                    logger.error(f"å¤„ç†URLå¤±è´¥ {current_url}: {e}")
                    self.state_manager.add_failed_url(current_url, str(e))
                    self.state_manager.update_stats(failed_downloads=1)
                
                # å¼ºåˆ¶ä¿å­˜å·²å¤„ç†URL
                self.state_manager.add_processed_url(current_url)
        
        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜çŠ¶æ€...")
        
        except Exception as e:
            logger.error(f"çˆ¬è™«è¿è¡Œé”™è¯¯: {e}")
        
        finally:
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - self.start_time
            self.state_manager.state["processing_time"] = processing_time
            
            # ä¿å­˜æœ€ç»ˆçŠ¶æ€
            self.state_manager.save_state()
            
            # ç”ŸæˆæŠ¥å‘Š
            self._generate_report()
            
            logger.info(f"çˆ¬è™«å®Œæˆ: æ€»è®¡å¤„ç† {pages_crawled} é¡µï¼Œè€—æ—¶ {processing_time:.2f} ç§’")
    
    def _initialize_urls(self, language, version):
        """åˆå§‹åŒ–ç›®æ ‡URLs"""
        version_config = self.language_versions.get(version, {})
        language_entry = version_config.get(language)

        if not language_entry:
            raise ValueError(f"ä¸æ”¯æŒçš„è¯­è¨€-ç‰ˆæœ¬ç»„åˆ: {language}-{version}")

        if isinstance(language_entry, str):
            language_entry = {'start_url': language_entry}

        start_url = language_entry['start_url']

        self.url_queue.append({
            'url': start_url,
            'text': 'ä¸»é¡µ',
            'source_url': None
        })

        for extra_url in language_entry.get('extra_urls', []):
            target_url = extra_url if extra_url.startswith('http') else urljoin(start_url, extra_url)
            if target_url == start_url:
                continue
            self.url_queue.append({
                'url': target_url,
                'text': 'é™„åŠ å…¥å£',
                'source_url': start_url
            })

        logger.info(f"å·²åˆå§‹åŒ–èµ·å§‹URLs: {len(self.url_queue)} ä¸ª")
    
    def _generate_report(self):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        state = self.state_manager.state
        
        report = f"""
        ğŸ“Š çˆ¬è™«æ‰§è¡ŒæŠ¥å‘Š
        ================
        
        æ€»ä½“ç»Ÿè®¡:
        - å¼€å§‹æ—¶é—´: {self.start_time}
        - å¤„ç†æ—¶é—´: {state['processing_time']:.2f} ç§’
        - å·²å¤„ç†URLs: {state['urls_processed']}
        - æˆåŠŸä¸‹è½½: {state['successful_downloads']}
        - è·³è¿‡å¯¼èˆªé¡µ: {state['landing_pages_skipped']}
        - ä¸‹è½½å¤±è´¥: {state['failed_downloads']}
        - è§£æé”™è¯¯: {state['parse_errors']}
        - é‡å¤å‘ç°: {state['duplicates_found']}
        - æ–°å»ºæ–‡ç« : {state['new_articles_created']}
        - æ›´æ–°æ–‡ç« : {state['existing_articles_updated']}
        
        æ€§èƒ½æŒ‡æ ‡:
        - å¹³å‡å“åº”æ—¶é—´: {state['processing_time'] / max(state['urls_processed'], 1):.2f} ç§’/é¡µ
        - é”™è¯¯ç‡: {(state['failed_downloads'] / max(state['urls_processed'], 1)) * 100:.2f}%
        - æˆåŠŸç‡: {(state['successful_downloads'] / max(state['urls_processed'], 1)) * 100:.2f}%
        
        é”™è¯¯è¯¦æƒ…:
        """
        
        if state['error_log']:
            report += "\\næœ€è¿‘çš„é”™è¯¯:"
            for error in state['error_log'][-10:]:  # æ˜¾ç¤ºæœ€è¿‘10ä¸ªé”™è¯¯
                report += f"\\n  - {error['url']}: {error['error']}"
        
        logger.info(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = f"logs/crawler_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

if __name__ == "__main__":
    # æµ‹è¯•çˆ¬è™«
    crawler = MSDManualsCrawler()
    
    # çˆ¬å–å°‘é‡é¡µé¢è¿›è¡Œæµ‹è¯•
    crawler.run(language='zh', version='home', max_pages=10)
