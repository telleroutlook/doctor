#!/usr/bin/env python3
"""
é»˜æ²™ä¸œè¯Šç–—æ‰‹å†Œçˆ¬è™«é¡¹ç›®åˆå§‹åŒ–è„šæœ¬
"""

import os
import sys
import subprocess
import json
from pathlib import Path

def create_project_structure():
    """åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„"""
    
    # å®šä¹‰é¡¹ç›®ç›®å½•ç»“æ„
    directories = [
        'config',
        'database/migrations',
        'crawler/spiders',
        'parsers',
        'processors',
        'api',
        'web_interface/static/css',
        'web_interface/static/js',
        'web_interface/templates',
        'tests',
        'docs',
        'scripts',
        'logs',
        'data/raw',
        'data/processed',
        'data/backup'
    ]
    
    # åˆ›å»ºç›®å½•
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        print(f"âœ… åˆ›å»ºç›®å½•: {directory}")

def setup_database():
    """è®¾ç½®æ•°æ®åº“é…ç½®æ–‡ä»¶"""
    
    db_config = {
        "host": "localhost",
        "port": 3306,
        "username": "root", 
        "password": "password",
        "database": "msd_manuals",
        "charset": "utf8mb4",
        "pool_size": 10,
        "pool_recycle": 3600,
        "pool_pre_ping": True
    }
    
    with open('config/database_config.py', 'w', encoding='utf-8') as f:
        f.write(f'''# æ•°æ®åº“é…ç½®
DATABASE_CONFIG = {json.dumps(db_config, indent=4, ensure_ascii=False)}

# SQLiteå¤‡ç”¨é…ç½®ï¼ˆç”¨äºæµ‹è¯•ï¼‰
SQLITE_CONFIG = {{
    "database": "msd_manuals.db",
    "timeout": 30,
    "check_same_thread": False
}}

class DatabaseManager:
    def __init__(self, config_type="mysql"):
        self.config_type = config_type
        if config_type == "mysql":
            import mysql.connector
            self.config = DATABASE_CONFIG
        else:
            import sqlite3
            self.config = SQLITE_CONFIG
    
    def get_connection(self):
        if self.config_type == "mysql":
            import mysql.connector
            return mysql.connector.connect(**self.config)
        else:
            import sqlite3
            return sqlite3.connect(self.config["database"])
''')
    
    print("âœ… åˆ›å»ºæ•°æ®åº“é…ç½®æ–‡ä»¶")

def setup_crawler_config():
    """è®¾ç½®çˆ¬è™«é…ç½®æ–‡ä»¶"""
    
    crawler_config = '''# çˆ¬è™«é…ç½®
import random

# çˆ¬è™«åŸºç¡€é…ç½®
CRAWLER_CONFIG = {
    "max_workers": 3,
    "delay_between_requests": 5.0,
    "max_retries": 3,
    "timeout": 30,
    "respect_robots_txt": True,
    "download_timeout": 60,
    "download_delay": 5,
    "randomize_download_delay": True,
    "download_delay_range": (4, 6),
}

# ç”¨æˆ·ä»£ç†è½®æ¢
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0"
]

# è¯·æ±‚å¤´é…ç½®
DEFAULT_HEADERS = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5,zh-CN;q=0.3",
    "Accept-Encoding": "gzip, deflate",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache"
}

# åŸŸåç‰¹å®šé…ç½®
DOMAIN_CONFIGS = {
    "www.msdmanuals.com": {
        "delay": 5.0,
        "max_concurrent": 3,
        "priority": "high"
    },
    "www.msdmanuals.cn": {
        "delay": 6.0,
        "max_concurrent": 2,
        "priority": "high"
    },
    "www.msdvetmanual.com": {
        "delay": 7.0,
        "max_concurrent": 1,
        "priority": "medium"
    }
}

# é‡è¯•é…ç½®
RETRY_CONFIG = {
    "max_retries": 3,
    "retry_delay_range": (1, 5),
    "backoff_factor": 2,
    "retry_status_codes": [429, 500, 502, 503, 504],
    "give_up_status_codes": [403, 404, 451]
}

# çŠ¶æ€æ–‡ä»¶é…ç½®
STATE_CONFIG = {
    "state_file": "crawler_state.json",
    "save_interval": 100,
    "checkpoints_dir": "checkpoints",
    "backup_enabled": True,
    "backup_interval": 500
}
'''
    
    with open('config/crawler_config.py', 'w', encoding='utf-8') as f:
        f.write(crawler_config)
    
    print("âœ… åˆ›å»ºçˆ¬è™«é…ç½®æ–‡ä»¶")

def setup_requirements():
    """åˆ›å»ºrequirements.txtæ–‡ä»¶"""
    
    requirements = '''# æ ¸å¿ƒçˆ¬è™«æ¡†æ¶
scrapy>=2.11.0
scrapy-redis>=0.7.0
scrapy-splash>=0.3.0

# æ•°æ®åº“
mysql-connector-python>=8.0.32
sqlalchemy>=2.0.0
alembic>=1.12.0
pymongo>=4.5.0

# æ•°æ®å¤„ç†
beautifulsoup4>=4.12.0
lxml>=4.9.0
nltk>=3.8.0
spacy>=3.6.0
pandas>=2.0.0
numpy>=1.24.0

# æ–‡æœ¬å¤„ç†
regex>=2023.0.0
jieba>=0.42.1
wordcloud>=1.9.2

# HTTPå’Œå¼‚æ­¥
requests>=2.31.0
aiohttp>=3.8.0
httpx>=0.24.0

# æ—¥å¿—å’Œç›‘æ§
loguru>=0.7.0
prometheus-client>=0.16.0

# Webæ¡†æ¶
flask>=2.3.0
flask-cors>=4.0.0
gunicorn>=21.0.0

# æµ‹è¯•
pytest>=7.4.0
pytest-cov>=4.1.0
pytest-asyncio>=0.21.0

# å·¥å…·åº“
python-dotenv>=1.0.0
pytz>=2023.3
pydantic>=2.3.0
click>=8.1.0
colorama>=0.4.6
'''
    
    with open('requirements.txt', 'w', encoding='utf-8') as f:
        f.write(requirements)
    
    print("âœ… åˆ›å»ºrequirements.txtæ–‡ä»¶")

def create_main_script():
    """åˆ›å»ºä¸»ç¨‹åºå…¥å£"""
    
    main_script = '''#!/usr/bin/env python3
"""
é»˜æ²™ä¸œè¯Šç–—æ‰‹å†Œçˆ¬è™«ç³»ç»Ÿä¸»ç¨‹åº
"""

import sys
import os
import argparse
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from crawler.main_crawler import MSDManualsCrawler
from database.setup_database import setup_database
from api.server import start_api_server
from web_interface.app import create_search_interface

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/crawler.log'),
            logging.StreamHandler(sys.stdout)
        ]
    )

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é»˜æ²™ä¸œè¯Šç–—æ‰‹å†Œçˆ¬è™«ç³»ç»Ÿ')
    parser.add_argument('command', choices=['crawl', 'setup-db', 'api', 'search', 'test'], 
                       help='è¦æ‰§è¡Œçš„å‘½ä»¤')
    parser.add_argument('--config', default='config/crawler_config.py', 
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', default='data/output', 
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--language', default='en', 
                       help='çˆ¬å–è¯­è¨€ç‰ˆæœ¬ (en, zh, fr, etc.)')
    parser.add_argument('--version', default='home', 
                       help='çˆ¬å–ç‰ˆæœ¬ (home, professional, veterinary)')
    parser.add_argument('--max-pages', type=int, default=1000, 
                       help='æœ€å¤§çˆ¬å–é¡µé¢æ•°')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger = logging.getLogger(__name__)
    
    if args.command == 'crawl':
        logger.info("å¼€å§‹çˆ¬å–æ•°æ®...")
        crawler = MSDManualsCrawler(config_path=args.config)
        crawler.run(
            language=args.language,
            version=args.version,
            max_pages=args.max_pages,
            output_dir=args.output
        )
        
    elif args.command == 'setup-db':
        logger.info("åˆå§‹åŒ–æ•°æ®åº“...")
        setup_database()
        
    elif args.command == 'api':
        logger.info("å¯åŠ¨APIæœåŠ¡...")
        start_api_server()
        
    elif args.command == 'search':
        logger.info("å¯åŠ¨æœç´¢ç•Œé¢...")
        create_search_interface()
        
    elif args.command == 'test':
        logger.info("è¿è¡Œæµ‹è¯•...")
        import subprocess
        result = subprocess.run(['pytest', '-v'], cwd=project_root)
        sys.exit(result.returncode)

if __name__ == '__main__':
    main()
'''
    
    with open('main.py', 'w', encoding='utf-8') as f:
        f.write(main_script)
    
    # è®¾ç½®æ‰§è¡Œæƒé™
    os.chmod('main.py', 0o755)
    
    print("âœ… åˆ›å»ºä¸»ç¨‹åºå…¥å£")

def create_database_models():
    """åˆ›å»ºæ•°æ®åº“æ¨¡å‹"""
    
    models_script = '''"""
æ•°æ®åº“æ¨¡å‹å®šä¹‰
"""

from sqlalchemy import Column, Integer, String, Text, DateTime, Float, JSON, ForeignKey, Index, UniqueConstraint
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime

Base = declarative_base()

class Article(Base):
    """æ–‡ç« è¡¨"""
    __tablename__ = 'articles'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    url = Column(String(1000), unique=True, nullable=False, index=True)
    title = Column(String(500), nullable=False, index=True)
    subtitle = Column(String(500))
    category = Column(String(100), index=True)
    subcategory = Column(String(100))
    content = Column(Text)
    content_html = Column(Text)
    excerpt = Column(Text)
    version = Column(String(20), default='home', index=True)
    language = Column(String(10), default='en', index=True)
    author = Column(String(200))
    last_reviewed = Column(DateTime)
    published_date = Column(DateTime)
    word_count = Column(Integer, default=0)
    hash_content = Column(String(64))
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # ç´¢å¼•
    __table_args__ = (
        Index('idx_category_language', 'category', 'language'),
        Index('idx_version_language', 'version', 'language'),
        Index('idx_content_length', 'word_count'),
    )
    
    # å…³ç³»
    medical_terms = relationship("MedicalTerm", secondary="article_terms", back_populates="articles")
    
class Category(Base):
    """åˆ†ç±»è¡¨"""
    __tablename__ = 'categories'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String(200), nullable=False)
    slug = Column(String(200), nullable=False, index=True)
    parent_id = Column(Integer, ForeignKey('categories.id'))
    level = Column(Integer, default=1)
    description = Column(Text)
    article_count = Column(Integer, default=0)
    version = Column(String(20), default='home', index=True)
    language = Column(String(10), default='en', index=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # çº¦æŸ
    __table_args__ = (
        UniqueConstraint('slug', 'version', 'language', name='uk_slug_version_lang'),
        Index('idx_parent', 'parent_id'),
    )
    
    # å…³ç³»
    parent = relationship("Category", remote_side=[id], backref="children")

class MedicalTerm(Base):
    """åŒ»å­¦æœ¯è¯­è¡¨"""
    __tablename__ = 'medical_terms'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    term = Column(String(300), nullable=False, index=True)
    definition = Column(Text)
    synonyms = Column(JSON)  # JSONæ•°ç»„
    category = Column(String(100), index=True)  # ç—‡çŠ¶, è¯Šæ–­, æ²»ç–—, è¯ç‰©ç­‰
    icd_code = Column(String(20))
    umls_id = Column(String(20))
    frequency_score = Column(Float, default=0.0)
    related_articles = Column(JSON)  # JSONæ•°ç»„
    version = Column(String(20), default='home', index=True)
    language = Column(String(10), default='en', index=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # çº¦æŸ
    __table_args__ = (
        UniqueConstraint('term', 'version', 'language', name='uk_term_version_lang'),
        Index('idx_category', 'category'),
    )
    
    # å…³ç³»
    articles = relationship("Article", secondary="article_terms", back_populates="medical_terms")

class ArticleTerm(Base):
    """æ–‡ç« æœ¯è¯­å…³è”è¡¨"""
    __tablename__ = 'article_terms'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    article_id = Column(Integer, ForeignKey('articles.id'), nullable=False)
    term_id = Column(Integer, ForeignKey('medical_terms.id'), nullable=False)
    frequency = Column(Integer, default=1)
    position = Column(JSON)  # JSONæ•°ç»„ï¼Œè®°å½•ä½ç½®ä¿¡æ¯
    
    # ç´¢å¼•
    __table_args__ = (
        UniqueConstraint('article_id', 'term_id', name='uk_article_term'),
        Index('idx_article', 'article_id'),
        Index('idx_term', 'term_id'),
    )

class Drug(Base):
    """è¯ç‰©ä¿¡æ¯è¡¨"""
    __tablename__ = 'drugs'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    generic_name = Column(String(300), index=True)
    brand_names = Column(JSON)  # JSONæ•°ç»„
    drug_class = Column(String(200), index=True)
    description = Column(Text)
    indications = Column(Text)  # é€‚åº”ç—‡
    contraindications = Column(Text)  # ç¦å¿Œç—‡
    dosage = Column(Text)
    side_effects = Column(Text)
    interactions = Column(Text)
    article_id = Column(Integer, ForeignKey('articles.id'))
    version = Column(String(20), default='home', index=True)
    language = Column(String(10), default='en', index=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # å…³ç³»
    article = relationship("Article")

class SearchLog(Base):
    """æœç´¢æ—¥å¿—è¡¨"""
    __tablename__ = 'search_logs'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    query = Column(String(500))
    results_count = Column(Integer, default=0)
    execution_time = Column(Float, default=0.0)
    user_agent = Column(String(200))
    ip_address = Column(String(50))
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # ç´¢å¼•
    __table_args__ = (
        Index('idx_query', 'query'),
        Index('idx_date', 'created_at'),
    )
'''
    
    with open('database/models.py', 'w', encoding='utf-8') as f:
        f.write(models_script)
    
    print("âœ… åˆ›å»ºæ•°æ®åº“æ¨¡å‹")

def create_readme():
    """åˆ›å»ºREADMEæ–‡ä»¶"""
    
    readme_content = '''# é»˜æ²™ä¸œè¯Šç–—æ‰‹å†Œçˆ¬è™«ç³»ç»Ÿ

ä¸€ä¸ªé«˜æ•ˆã€åˆè§„çš„åŒ»å­¦æ–‡çŒ®æ•°æ®æŠ“å–ä¸æ£€ç´¢ç³»ç»Ÿã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

- **åˆè§„æŠ“å–**: ä¸¥æ ¼éµå®ˆrobots.txtæ”¿ç­–ï¼Œæ™ºèƒ½é¢‘ç‡æ§åˆ¶
- **åŒ»å­¦è§£æ**: ä¸“é—¨ä¼˜åŒ–çš„åŒ»å­¦å†…å®¹è§£æå™¨
- **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒ16ç§è¯­è¨€ç‰ˆæœ¬çš„æŠ“å–
- **æ™ºèƒ½æœç´¢**: åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†çš„å…¨æ–‡æ£€ç´¢
- **å¢é‡æ›´æ–°**: æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œå¢é‡æ›´æ–°
- **è´¨é‡ä¿è¯**: å®Œæ•´çš„æ•°æ®è´¨é‡æ£€æŸ¥æœºåˆ¶

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
# å®‰è£…Pythonä¾èµ–
pip install -r requirements.txt

# åˆå§‹åŒ–æ•°æ®åº“
python main.py setup-db
```

## ğŸ•·ï¸ å¿«é€Ÿå¼€å§‹

### çˆ¬å–æ•°æ®
```bash
# çˆ¬å–è‹±æ–‡æ¶ˆè´¹è€…ç‰ˆæ•°æ®
python main.py crawl --language en --version home --max-pages 1000

# çˆ¬å–ä¸­æ–‡ä¸“ä¸šç‰ˆæ•°æ®
python main.py crawl --language zh --version professional --max-pages 500

# æŒ‡å®šè¾“å‡ºç›®å½•
python main.py crawl --language en --version home --output ./output --max-pages 2000
```

### å¯åŠ¨æœåŠ¡
```bash
# å¯åŠ¨æœç´¢API
python main.py api

# å¯åŠ¨æœç´¢ç•Œé¢
python main.py search
```

### è¿è¡Œæµ‹è¯•
```bash
python main.py test
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
msd_crawler_project/
â”œâ”€â”€ config/                 # é…ç½®æ–‡ä»¶
â”œâ”€â”€ database/               # æ•°æ®åº“ç›¸å…³
â”œâ”€â”€ crawler/                # çˆ¬è™«æ ¸å¿ƒä»£ç 
â”œâ”€â”€ parsers/                # å†…å®¹è§£æå™¨
â”œâ”€â”€ processors/             # æ•°æ®å¤„ç†å™¨
â”œâ”€â”€ api/                    # APIæœåŠ¡
â”œâ”€â”€ web_interface/          # Webç•Œé¢
â”œâ”€â”€ tests/                  # æµ‹è¯•ä»£ç 
â”œâ”€â”€ data/                   # æ•°æ®æ–‡ä»¶
â”œâ”€â”€ docs/                   # æ–‡æ¡£
â””â”€â”€ scripts/                # è„šæœ¬å·¥å…·
```

## âš™ï¸ é…ç½®è¯´æ˜

### æ•°æ®åº“é…ç½®
- æ”¯æŒMySQLå’ŒSQLite
- è‡ªåŠ¨åˆ›å»ºå¿…è¦çš„è¡¨ç»“æ„
- æ”¯æŒå…¨æ–‡æœç´¢ç´¢å¼•

### çˆ¬è™«é…ç½®
- å¯é…ç½®å¹¶å‘æ•°å’Œå»¶è¿Ÿ
- æ™ºèƒ½é‡è¯•æœºåˆ¶
- æ–­ç‚¹ç»­ä¼ æ”¯æŒ

## ğŸ“Š æ•°æ®ç»“æ„

### ä¸»è¦è¡¨
- `articles`: æ–‡ç« å†…å®¹
- `categories`: åŒ»å­¦åˆ†ç±»
- `medical_terms`: åŒ»å­¦æœ¯è¯­
- `drugs`: è¯ç‰©ä¿¡æ¯
- `search_logs`: æœç´¢æ—¥å¿—

### æœç´¢åŠŸèƒ½
- å…¨æ–‡æœç´¢
- æŒ‰åˆ†ç±»ç­›é€‰
- å…³é”®è¯é«˜äº®
- ç›¸å…³æ€§æ’åº

## ğŸ› ï¸ æŠ€æœ¯æ ˆ

- **çˆ¬è™«æ¡†æ¶**: Scrapy
- **æ•°æ®åº“**: MySQL/SQLite
- **æ–‡æœ¬å¤„ç†**: NLTK, spaCy
- **Webæ¡†æ¶**: Flask
- **å‰ç«¯**: HTML5 + JavaScript

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ç”¨äºå­¦æœ¯ç ”ç©¶ç›®çš„ã€‚è¯·éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„å’Œç½‘ç«™ä½¿ç”¨æ¡æ¬¾ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›é¡¹ç›®ï¼

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·é€šè¿‡GitHub Issuesè”ç³»æˆ‘ä»¬ã€‚
'''
    
    with open('README.md', 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print("âœ… åˆ›å»ºREADME.mdæ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åˆå§‹åŒ–é»˜æ²™ä¸œè¯Šç–—æ‰‹å†Œçˆ¬è™«é¡¹ç›®...")
    
    # åˆ›å»ºé¡¹ç›®ç»“æ„
    create_project_structure()
    
    # è®¾ç½®é…ç½®æ–‡ä»¶
    setup_database()
    setup_crawler_config()
    setup_requirements()
    
    # åˆ›å»ºæ ¸å¿ƒæ–‡ä»¶
    create_main_script()
    create_database_models()
    create_readme()
    
    print("\\nâœ… é¡¹ç›®åˆå§‹åŒ–å®Œæˆï¼")
    print("\\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print("1. å®‰è£…ä¾èµ–: pip install -r requirements.txt")
    print("2. åˆå§‹åŒ–æ•°æ®åº“: python main.py setup-db")
    print("3. å¼€å§‹çˆ¬å–: python main.py crawl --language en --version home --max-pages 100")
    print("\\nğŸ“š æŸ¥çœ‹README.mdè·å–è¯¦ç»†ä½¿ç”¨è¯´æ˜")

if __name__ == '__main__':
    main()
